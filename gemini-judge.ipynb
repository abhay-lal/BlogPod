{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import pandas as pd \n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blog Assessment:\n"
     ]
    }
   ],
   "source": [
    "# Configure the Gemini model API key\n",
    "genai.configure(api_key=\"API_KEY\")\n",
    "model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "def assess_blog(blog_text):\n",
    "    \"\"\"\n",
    "    Function to assess a blog using the Gemini 1.5 Flash model with strict evaluation criteria.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Conduct a rigorous evaluation of the following blog text based on the specified parameters. Apply strict marking criteria and penalize heavily for any shortcomings:\n",
    "\n",
    "    Blog Text:\n",
    "    \"{blog_text}\"\n",
    "\n",
    "    Evaluation Parameters:\n",
    "    1. Clarity (1-10): Assess how easily understandable the content is. Penalize for any ambiguity or confusion.\n",
    "    2. Grammar and Syntax (1-10): Scrutinize for any grammatical errors or awkward phrasing. Even minor mistakes should result in point deductions.\n",
    "    3. Tone Appropriateness (1-10): Evaluate if the tone consistently matches a general audience. Any inappropriate shifts in tone should be penalized.\n",
    "    4. Sentence Structure and Flow (1-10): Analyze the smoothness and logical progression of ideas. Penalize for any choppiness or lack of coherence.\n",
    "    5. Engagement (1-10): Determine how well the blog captures and maintains reader interest. Lack of engaging elements should result in lower scores.\n",
    "    6. Conciseness (1-10): Assess for unnecessary verbosity or repetition. Penalize for any superfluous content.\n",
    "\n",
    "    Scoring Guidelines:\n",
    "    - Use the full range of scores (1-10) effectively.\n",
    "    - Score 9-10: Exceptional, near-perfect performance\n",
    "    - Score 7-8: Good performance with minor issues\n",
    "    - Score 5-6: Average performance with noticeable flaws\n",
    "    - Score 3-4: Poor performance with significant issues\n",
    "    - Score 1-2: Severely lacking in this aspect\n",
    "\n",
    "    Provide your assessment in the following strict format:\n",
    "    [Clarity score],[Grammar and Syntax score],[Tone Appropriateness score],[Sentence Structure and Flow score],[Engagement score],[Conciseness score]\n",
    "    Suggestions: [Detailed suggestions for improvement, highlighting specific areas that led to score deductions]\n",
    "\n",
    "    Ensure all scores are integers. Separate scores with commas only, no spaces. Provide specific, actionable suggestions for improvement.\n",
    "    \"\"\"\n",
    "\n",
    "    response = model.generate_content(prompt)\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import csv\n",
    "if 'assessment_70b' not in df.columns: # change for every file \n",
    "    df['assessment_70b'] = None\n",
    "# Function to assess and save blog dynamically\n",
    "def assess_blog_save(text, id, column_name, output_file):\n",
    "    \"\"\"Assess the blog text and save the results dynamically to a CSV file.\"\"\"\n",
    "    # Assess the blog text\n",
    "    assessment = assess_blog(text)\n",
    "    \n",
    "    # Append result to the CSV file\n",
    "    with open(output_file, 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow([id, assessment])\n",
    "    \n",
    "    return assessment\n",
    "\n",
    "# Initialize output files with headers\n",
    "with open('assessment_70b.csv', 'a', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['id', 'assessment_70b'])\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def process_item_with_retry(item):\n",
    "    batch_size = 10  \n",
    "    for start in range(0, len(df), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = df.iloc[start:end]\n",
    "        try:\n",
    "            for idx, row in tqdm(batch.iterrows(), total=len(batch), desc=f\"Batch {start//batch_size + 1}\"):\n",
    "                try:\n",
    "                    if pd.notna(row['assessment_8b']):\n",
    "                        df.loc[idx, 'assessment_8b'] = row['assessment_8b']\n",
    "                    else:\n",
    "                        df.loc[idx, 'assessment_8b'] = assess_blog_save(\n",
    "                            row['meta_llama_3.1-8b-instruct'], \n",
    "                            row['id'], \n",
    "                            'assessment_8b', \n",
    "                            'assessment_8b.csv'\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing row {idx}: {str(e)}\")\n",
    "                    time.sleep(5)  \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Major error occurred: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in batch {start//batch_size + 1}: {str(e)}\")\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def process_item_with_retry(item):\n",
    "    batch_size = 10  \n",
    "    for start in range(0, len(df), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = df.iloc[start:end]\n",
    "        try:\n",
    "            for idx, row in tqdm(batch.iterrows(), total=len(batch), desc=f\"Batch {start//batch_size + 1}\"):\n",
    "                try:\n",
    "                    if pd.notna(row['assessment_405b']):\n",
    "                        df.loc[idx, 'assessment_405b'] = row['assessment_405b']\n",
    "                    else:\n",
    "                        df.loc[idx, 'assessment_405b'] = assess_blog_save(\n",
    "                            row['meta_llama_3.1-405b-instruct'], \n",
    "                            row['id'], \n",
    "                            'assessment_405b', \n",
    "                            'assessment_405b.csv'\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing row {idx}: {str(e)}\")\n",
    "                    time.sleep(5)  \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Major error occurred: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in batch {start//batch_size + 1}: {str(e)}\")\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def process_item_with_retry(item):\n",
    "    batch_size = 10 \n",
    "    for start in range(0, len(df), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch = df.iloc[start:end]\n",
    "        try:\n",
    "            for idx, row in tqdm(batch.iterrows(), total=len(batch), desc=f\"Batch {start//batch_size + 1}\"):\n",
    "                try:\n",
    "                    if pd.notna(row['assessment_70b']):\n",
    "                        df.loc[idx, 'assessment_70b'] = row['assessment_70b']\n",
    "                    else:\n",
    "                        df.loc[idx, 'assessment_70b'] = assess_blog_save(\n",
    "                            row['meta_llama_3.1-70b-instruct'], \n",
    "                            row['id'], \n",
    "                            'assessment_70b', \n",
    "                            'assessment_70b.csv'\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error processing row {idx}: {str(e)}\")\n",
    "                    time.sleep(5) \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Major error occurred: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error in batch {start//batch_size + 1}: {str(e)}\")\n",
    "            time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|██████████| 10/10 [00:00<00:00, 4427.17it/s]\n",
      "Batch 2: 100%|██████████| 10/10 [00:00<00:00, 6588.60it/s]\n",
      "Batch 3:  70%|███████   | 7/10 [00:05<00:02,  1.30it/s]ERROR:root:Error processing row 27: 504 Deadline Exceeded\n",
      "Batch 3:  80%|████████  | 8/10 [10:10<03:26, 103.19s/it]ERROR:root:Error processing row 28: 504 Deadline Exceeded\n",
      "Batch 3: 100%|██████████| 10/10 [20:19<00:00, 121.93s/it]\n",
      "Batch 4:  50%|█████     | 5/10 [00:24<00:24,  4.92s/it]ERROR:root:Error processing row 35: 504 Deadline Exceeded\n",
      "Batch 4:  90%|█████████ | 9/10 [10:43<01:09, 69.17s/it] ERROR:root:Error processing row 39: 504 Deadline Exceeded\n",
      "Batch 4: 100%|██████████| 10/10 [20:49<00:00, 124.90s/it]\n",
      "Batch 5:  10%|█         | 1/10 [00:05<00:46,  5.16s/it]ERROR:root:Error processing row 41: 504 Deadline Exceeded\n",
      "Batch 5:  60%|██████    | 6/10 [10:28<03:34, 53.65s/it] ERROR:root:Error processing row 46: 504 Deadline Exceeded\n",
      "Batch 5: 100%|██████████| 10/10 [20:47<00:00, 124.78s/it]\n",
      "Batch 6:  60%|██████    | 6/10 [00:25<00:15,  3.86s/it]ERROR:root:Error processing row 56: 504 Deadline Exceeded\n",
      "Batch 6: 100%|██████████| 10/10 [10:45<00:00, 64.55s/it]\n",
      "Batch 7:  40%|████      | 4/10 [00:19<00:30,  5.04s/it]ERROR:root:Error processing row 64: 504 Deadline Exceeded\n",
      "Batch 7:  70%|███████   | 7/10 [10:33<05:01, 100.61s/it]ERROR:root:Error processing row 67: 504 Deadline Exceeded\n",
      "Batch 7: 100%|██████████| 10/10 [20:48<00:00, 124.85s/it]\n",
      "Batch 8:   0%|          | 0/10 [00:00<?, ?it/s]ERROR:root:Error processing row 70: 504 Deadline Exceeded\n",
      "Batch 8:  60%|██████    | 6/10 [10:28<02:35, 38.91s/it]   ERROR:root:Error processing row 76: 504 Deadline Exceeded\n",
      "Batch 8: 100%|██████████| 10/10 [20:47<00:00, 124.73s/it]\n",
      "Batch 9:  30%|███       | 3/10 [00:14<00:34,  4.99s/it]ERROR:root:Error processing row 83: 504 Deadline Exceeded\n",
      "Batch 9:  70%|███████   | 7/10 [10:33<03:36, 72.22s/it] ERROR:root:Error processing row 87: 504 Deadline Exceeded\n",
      "Batch 9:  80%|████████  | 8/10 [20:38<08:03, 241.85s/it]ERROR:root:Error processing row 88: 504 Deadline Exceeded\n",
      "Batch 9: 100%|██████████| 10/10 [30:50<00:00, 185.02s/it]\n",
      "Batch 10:  10%|█         | 1/10 [00:04<00:39,  4.44s/it]ERROR:root:Error processing row 91: 504 Deadline Exceeded\n",
      "Batch 10:  20%|██        | 2/10 [10:09<47:41, 357.72s/it]ERROR:root:Error processing row 92: 504 Deadline Exceeded\n",
      "Batch 10:  40%|████      | 4/10 [20:20<28:43, 287.28s/it]ERROR:root:Error processing row 94: 504 Deadline Exceeded\n",
      "Batch 10:  80%|████████  | 8/10 [30:38<04:09, 124.70s/it]ERROR:root:Error processing row 98: 504 Deadline Exceeded\n",
      "Batch 10: 100%|██████████| 10/10 [40:50<00:00, 245.01s/it]\n"
     ]
    }
   ],
   "source": [
    "process_item_with_retry(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|██████████| 10/10 [00:00<00:00, 4673.32it/s]\n",
      "Batch 2: 100%|██████████| 10/10 [00:00<00:00, 3930.56it/s]\n",
      "Batch 3: 100%|██████████| 10/10 [00:08<00:00,  1.16it/s]\n",
      "Batch 4: 100%|██████████| 10/10 [00:09<00:00,  1.07it/s]\n",
      "Batch 5: 100%|██████████| 10/10 [00:08<00:00,  1.13it/s]\n",
      "Batch 6: 100%|██████████| 10/10 [00:05<00:00,  1.91it/s]\n",
      "Batch 7:   0%|          | 0/10 [00:00<?, ?it/s]ERROR:root:Error processing row 64: 504 Deadline Exceeded\n",
      "Batch 7: 100%|██████████| 10/10 [10:10<00:00, 61.02s/it]\n",
      "Batch 8: 100%|██████████| 10/10 [00:10<00:00,  1.04s/it]\n",
      "Batch 9:  40%|████      | 4/10 [00:04<00:06,  1.15s/it]ERROR:root:Error processing row 87: 504 Deadline Exceeded\n",
      "Batch 9: 100%|██████████| 10/10 [10:14<00:00, 61.41s/it]\n",
      "Batch 10:  30%|███       | 3/10 [00:10<00:25,  3.70s/it]ERROR:root:Error processing row 94: 504 Deadline Exceeded\n",
      "Batch 10: 100%|██████████| 10/10 [10:20<00:00, 62.05s/it]\n"
     ]
    }
   ],
   "source": [
    "process_item_with_retry(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch 1: 100%|██████████| 10/10 [00:00<00:00, 1725.34it/s]\n",
      "Batch 2: 100%|██████████| 10/10 [00:00<00:00, 6132.02it/s]\n",
      "Batch 3: 100%|██████████| 10/10 [00:00<00:00, 3201.03it/s]\n",
      "Batch 4: 100%|██████████| 10/10 [00:00<00:00, 6648.13it/s]\n",
      "Batch 5: 100%|██████████| 10/10 [00:00<00:00, 5321.37it/s]\n",
      "Batch 6: 100%|██████████| 10/10 [00:00<00:00, 2558.13it/s]\n",
      "Batch 7: 100%|██████████| 10/10 [00:06<00:00,  1.54it/s]\n",
      "Batch 8: 100%|██████████| 10/10 [00:00<00:00, 4144.16it/s]\n",
      "Batch 9: 100%|██████████| 10/10 [00:05<00:00,  1.75it/s]\n",
      "Batch 10: 100%|██████████| 10/10 [00:04<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "process_item_with_retry(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
